{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nosferatu0205/BanglaSpellCheck/blob/main/AI_Project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import functools\n",
        "import re\n",
        "import string\n"
      ],
      "metadata": {
        "id": "f0A9yUrKFJz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rule_words = ['ই', 'ও', 'তো', 'কে', 'তে', 'রা', 'চ্ছি', 'চ্ছিল', 'চ্ছে', 'চ্ছিস', 'চ্ছিলেন', 'চ্ছ', 'য়েছে', 'েছ', 'েছে',\n",
        "              'েছেন', 'রছ', 'রব', 'েল', 'েলো', 'ওয়া', 'েয়ে', 'য়', 'য়ে', 'েয়েছিল', 'েছিল', 'য়েছিল', 'েয়েছিলেন',\n",
        "              'ে.েছিলেন', 'েছিলেন', 'লেন', 'দের', 'ে.ে', 'ের', 'ার', 'েন', 'বেন', 'িস', 'ছিস', 'ছিলি', 'ছি', 'ছে', 'লি',\n",
        "              'বি', 'ে', 'টি', 'টির', 'েরটা', 'েরটার', 'টা', 'টার', 'গুলো', 'গুলোর', 'েরগুলো', 'েরগুলোর'\n",
        "              'যোগ্য','কেই','েও','সহ','রা','ভাবে','কারি','কৃত','ই','কে','র','কি','েই','ভাবে','গুলো',\n",
        "              'তে','েতে','গন','মুলক','সুচক','টুকু','টুকুই','গুলি','পদ','সমুহ','সংক্রান্ত','সংলগ্ন','সংশ্লিষ্ট',\n",
        "              'সুত্রে','রুপে','ানুসারে','ানুযায়ি','তত্ত্','ি','মুখি','প্রতি','ভাবে','য়'\n",
        "              ]\n",
        "rule_dict = {\"রছ\":\"র\",\"রব\":\"র\",\"েয়ে\":\"া\",\"েয়েছিল\":\"া\",\"েয়েছিলেন\":\"া\",\"ে.েছিলেন\":\"া.\",\"ে.ে\":\"া.\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "HLdc7vw8FEW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bn2en={'0985':'a','0986':'aa','0987':'i','0988':'ii',\n",
        "           '0989':'u','098A':'uu','098B':'r','09E0':'rr',\n",
        "           '098C':'l','09E1':'ll','098F':'e','0990':'ai',\n",
        "           '0993':'o','0994':'au',\n",
        "           '0995':'ka','0996':'kha','0997':'ga','0998':'gha',\n",
        "           '0999':'nga','099A':'ca','099B':'cha','099C':'ja',\n",
        "           '099D':'jha','099E':'nya','099F':'tta', '09A0':'ttha',\n",
        "           '09A1':'dda','09DC':'rra','09A2':'ddha','09DD':'rha',\n",
        "           '09A3':'nna','09A4':'ta','09A5':'tha','09A6':'da',\n",
        "           '09A7':'dha','09A8':'na','09AA':'pa','09AB':'pha',\n",
        "           '09AC':'ba','09AD':'bha','09AE':'ma','09AF':'ya',\n",
        "           '09DF':'yya','09B0':'ra','09B2':'la','09B6':'sha',\n",
        "           '09B7':'ssa','09B8':'sa','09B9':'ha','09CE':'ta',\n",
        "           '09BE':'aa','09BF':'i','09C0':'ii','09C1':'u',\n",
        "           '09C2':'uu','09C3':'r','09C4':'rr','09C7':'e',\n",
        "           '09C8':'ai', '09CB':'o','09CC':'au',\n",
        "           '09CD':'','09D7':'','09E6':'0',\n",
        "           '09E7':'1','09E8':'2','09E9':'3','09EA':'4',\n",
        "           '09EB':'5','09EC':'6','09ED':'7','09EE':'8',\n",
        "           '09EF':'9',\n",
        "           '09BC':'','0982':'n','0983':'','0981':''\n",
        "           }\n",
        "bn2enNum={\n",
        "        '09E6': '0',\n",
        "        '09E7': '1', '09E8': '2', '09E9': '3', '09EA': '4',\n",
        "        '09EB': '5', '09EC': '6', '09ED': '7', '09EE': '8',\n",
        "        '09EF': '9'\n",
        "    }\n",
        "bn2enPunc={\n",
        "        '09BE': 'a', '09BF': 'i', '09C0': 'i', '09C1': 'u',\n",
        "        '09C2': 'u', '09C3': 'r', '09C4': 'r', '09C7': 'e',\n",
        "        '09C8': 'ai', '09CB': 'o', '09CC': 'au','09B0':'ra'\n",
        "    }\n",
        "bn2enSuffix={\n",
        "        '09BE': 'a', '09B0':'ra','09CB': 'o','0993':'o','09C7': 'e'\n",
        "    }\n",
        "bn_norm={\n",
        "        '09C0':'\\u09bf','09C2':'\\u09c1','09C4':'\\u09c3','09A3':'\\u09a8','0988':'\\u0987','098A':'\\u0989'\n",
        "    }\n",
        "bn_serial={\n",
        "        '0985': 0, '0986': 1, '09BE': 2, '0987': 3, '09BF': 4,'0988': 5, '09C0': 6,\n",
        "        '0989': 7, '09C1': 8, '098A': 9, '09C2': 10,'098B': 11,'09C3': 12, '09E0': 13,'09C4': 14,\n",
        "        '098C': 15, '09E1': 16, '098F': 17,  '09C7': 18, '0990': 19,'09C8': 20,\n",
        "        '0993': 21, '09CB': 22, '0994': 23, '09CC': 24,\n",
        "        '0995': 25, '0996': 26, '0997': 27, '0998': 28,\n",
        "        '0999': 29, '099A': 30, '099B': 31, '099C': 32,\n",
        "        '099D': 33, '099E': 34, '099F': 35, '09A0': 36,\n",
        "        '09A1': 37, '09DC': 38, '09A2': 39, '09DD': 40,\n",
        "        '09A3': 41, '09A4': 42, '09A5': 43, '09A6': 44,\n",
        "        '09A7': 45, '09A8': 46, '09AA': 47, '09AB': 48,\n",
        "        '09AC': 49, '09AD': 50, '09AE': 51, '09AF': 52,\n",
        "        '09DF': 53, '09B0': 54, '09B2': 55, '09B6': 56,\n",
        "        '09B7': 57, '09B8': 58, '09B9': 59, '09CE': 60,\n",
        "        '09E6': 61,\n",
        "        '09E7': 62, '09E8': 63, '09E9': 64, '09EA': 65,\n",
        "        '09EB': 66, '09EC': 67, '09ED': 68, '09EE': 69,\n",
        "        '09EF': 70,\n",
        "        '09CD': 71, '09D7': 72,\n",
        "        '09BC': 73, '0982': 74, '0983': 75, '0981': 76\n",
        "      }"
      ],
      "metadata": {
        "id": "VJ2oEabSFzAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dust_removal(word):\n",
        "        s=\"\"\n",
        "        for c in word:\n",
        "            g = c.encode(\"unicode_escape\")\n",
        "            g = g.upper()\n",
        "            g = g[2:]\n",
        "            g = g.decode('utf-8')\n",
        "            if g in bn2en:\n",
        "                s+=c\n",
        "        return s"
      ],
      "metadata": {
        "id": "pBZ4Rm_1Ms5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vec=[]\n",
        "word_dict={}\n",
        "word_dict2={}"
      ],
      "metadata": {
        "id": "YnmMtH4kGpEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_dictionary():\n",
        "    try:\n",
        "        with open(\"/content/word2.txt\", \"r\", encoding='utf-8') as file1:\n",
        "            for word in file1:\n",
        "                word = word.replace('\\n', '')\n",
        "                word_vec.append(word)\n",
        "                word_dict[word] = 1\n",
        "    except UnicodeDecodeError:\n",
        "        with open(\"/content/word2.txt\", \"r\", encoding='latin-1') as file1:\n",
        "            for word in file1:\n",
        "                word = word.replace('\\n', '')\n",
        "                word_vec.append(word)\n",
        "                word_dict[word] = 1\n",
        "\n",
        "    try:\n",
        "        with open(\"/content/word3.txt\", \"r\", encoding='utf-8') as file2:\n",
        "            for word in file2:\n",
        "                word = word.replace('\\n', '')\n",
        "                word_dict2[word] = 1\n",
        "    except UnicodeDecodeError:\n",
        "        with open(\"/content/word3.txt\", \"r\", encoding='latin-1') as file2:\n",
        "            for word in file2:\n",
        "                word = word.replace('\\n', '')\n",
        "                word_dict2[word] = 1\n"
      ],
      "metadata": {
        "id": "4QlXdX1iGVXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_normalize(word):\n",
        "        s = \"\"\n",
        "        for c in word:\n",
        "            g = c.encode(\"unicode_escape\")\n",
        "            g = g.upper()\n",
        "            g = g[2:]\n",
        "            g = g.decode('utf-8')\n",
        "            if g in bn_norm:\n",
        "                g = bn_norm[g].encode().decode('utf-8')\n",
        "                s+=g\n",
        "                continue\n",
        "            s+=c\n",
        "        return s"
      ],
      "metadata": {
        "id": "QfqnYhkpF-sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zMj2Jq27HF1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7JWEzEVE0Vc"
      },
      "outputs": [],
      "source": [
        "def search(word):\n",
        "        if word_normalize(word) in word_dict:\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9jx4qhlWMDjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def normalize_tokenizer(sent):\n",
        "    sent=word_normalize(sent)\n",
        "    tokens=sent.split()\n",
        "    temp_tokens=[]\n",
        "    for i in tokens:\n",
        "        if len(dust_removal(i))==0:\n",
        "            continue\n",
        "        i=dust_removal(i)\n",
        "        temp_tokens.append(i)\n",
        "    return temp_tokens\n"
      ],
      "metadata": {
        "id": "Yze5zhr4L7rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dictionary()"
      ],
      "metadata": {
        "id": "wEN2bDkoHoKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dust_removal(\"অবস....\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DKGA5lqgM1bd",
        "outputId": "051d3aba-38bf-4973-eee8-70751cacebcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'অবস'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search(\"অবসবরপ্র\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYtDJ1P9IfoP",
        "outputId": "99927a96-380f-4569-8552-eb0855bd4b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_tokenizer(\"অবস অবসর\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDQd_ffFNXQm",
        "outputId": "e218cb84-bd04-4e08-f3e2-3382dec689e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['অবস', 'অবসর']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}